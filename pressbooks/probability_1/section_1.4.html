<!-- 1.4. Dependency and correlation -->
<h3>1.4 Dependency and correlation</h3>
<p class="indent" style="text-align: justify; text-indent: 2em">Data rarely lives in isolation. Often, even in the simplest case, one variable depends upon another. Rainfall influences crop yields, study hours affect exam results, in the same way the brightness of a star relates to its temperature, and atmospheric carbon levels affect global temperatures, just to list some examples. Hence, recognizing and describing such dependencies lies at the heart of descriptive statistics and prepares the way for predictive models.</p>

<div style="justify-content: center;align-items: flex-start;gap: 20px;margin-top: 10px">
<figure style="text-align: center"><img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/mean_std_hist-300x191.png" alt="" width="300" height="191" /><figcaption>Fig. 1.6 (a) Histogram of three samples drawn from a Gaussian distribution</figcaption></figure>
<figure style="text-align: center"><img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/mean_std_box-300x191.png" alt="" width="300" height="191" /><figcaption>Fig. 1.6 (b) Box plot of three samples drawn from a Gaussian distribution</figcaption></figure>
<figure style="text-align: center"><img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/mean_std_violin-300x191.png" alt="" width="300" height="191" /><figcaption>Fig. 1.6 (c) Violin of three samples drawn from a Gaussian distribution</figcaption></figure>
</div>

<p class="indent" style="text-align: justify; text-indent: 2em">The simplest and most widely studied form of dependency is \textit{linearity}. When one variable $y$ tends to increase in proportion to another $x$, the relation can be sketched as a straight line. In the language of calculus - also called some times \textit{analysis}, or \textit{regression} - the best-fitting line is expressed as
\[
y(x) = a x + b \; ,
\]

<p class="indent" style="text-align: justify; text-indent: 2em">The idea of correlation is a fundamental measure of association between two random variables, quantifying how strongly they vary together. The most widely used mathematical description is the \textit{Pearson correlation coefficient}, introduced by Karl Pearson in the 1890s, which is built upon the idea of covariance normalized by variability. For two random variables $x$ and $y$, the population correlation $\rho_{x, y}$ is defined as </p>
\[
y(x) = a x^2 + b x + c \; ,
\]
