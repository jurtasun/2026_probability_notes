<!-- 1.2. Central tendency and variation -->
<h3>1.2 Central tendency and variation</h3>
<p class="indent" style="text-align: justify; text-indent: 2em">Once observations have been collected, a natural question arises: what is the <em>central</em>, or <em>typical</em> value of this data set? Mathematical quantities that measure the central tendency will be useful to summarize our data with a single representative number, providing an immediate sense of location within the distribution.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">The <em>mean</em>, or <em>average,</em> is perhaps the most familiar measure of central tendency. Imagine we are doing an experiment where we measure some variable, and let's call it \(x\) for simplicity. \(x\) can be anything we could measure, like number of tomatoes in a bag, position at a given time, energy of some system, concentration of a specific substance, etc. Let's imagine we repeat the measurement \(n\) times, and we obtain the values \(x_1, x_2, \dots, x_n\). That will be our set of observations, or our <em>sample</em> \(\mathcal{S}\). We could simply write it as a list—or a <em>vector—</em>in the following way:</p>
\[
\mathcal{S} = \{x_1, x_2, \dots, x_n\} \; .
\]
<p style="text-align: justify">Keep in mind that from the mathematics perspective the word <em>vector</em> has a slightly different meaning, with subtleties related to algebraic operations and relations they should satisfy, but for the purpose of this course, where we prioritize above all simplicity, a vector and a list of numbers will be essentially the same thing. We can define a quantity called the <em>mean—</em>or <em>average</em>—of an arbitrary large sample of \(n\) observations, as the sum of all elements divided by the total. We will write it as \(\bar{x}\), and define it as follows:\[
\bar{x} = \frac{1}{n} (x_{1} + x_{2} + ... + x_{n}) \; .
\]</p>
We can write this in a slightly more compact way as a <em>summation</em>, as follows:
\[
\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i \; .
\]
<p style="text-align: justify">Here we denote the sum of all elements \(x_{i}\) with the greek letter \(\Sigma\), starting with the first one (\(x_1\), for \(i = 1\)) and until the last one (\(x_n\), for \(i = n\)). Both definitions for \(\bar{x}\) mean <em>exactly</em> the same thing, just written in different ways.</p>
<p style="text-align: justify;text-indent: 2em">Let's pause here for a second, and give a note about notation. Remember the difference we made at the very beginning between sample and population, as notations may differ between different books and literature sources. Normally, the sample mean is written just as \(\bar{x}\) just as we did, while for the full population of \(N\) elements \(x_1, x_2, \dots, x_N\)—before any sampling—the <em>population mean</em> is normally denoted as \(\mu\), and defined accordingly,</p>
\[
\mu = \frac{1}{N} \sum_{i = 1}^{N} x_{i} \; .
\]
<p style="text-align: justify">We will see more about the difference between sample mean and population mean when we discuss parameter estimation in Chapter 3. For now just keep in mind that \(\bar{x}\) is the mean of our sample of just \(n\) drawn observations, while \(\mu\) refers to the mean of the idealized, complete population.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">Let's illustrate with an example. Suppose we repeat a measurement three times, obtaining the results \(x_1 = 1\), \(x_2 = 2\), and \(x_3 = 3\). Our sample is then \(\mathcal{S} = \{1, 2, 3\}\), and the sample mean is computed by applying our definition,
\[
\bar{x} = \frac{1}{3} \sum_{i = 1}^{3} x_{i} = \frac{1}{3} (1 + 2 + 3) = 2 \; .
\]
As a warm-up exercise, try computing the same mean value for a second sample, let's say \(\mathcal{S} = \{4, 5, 6\}\). Substituting into the general expression we obtain
\[
\bar{x} = \frac{1}{3} \sum_{i = 1}^{3} x_{i} = \frac{1}{3} (4 + 5 + 6) = 5 \; .
\]
The mean captures information about the "central" value, where most events cluster. Although useful, it is sensitive to extreme values or <em>outliers</em>, which motivates the definition of additional, more robust measures of central tendency.</p>
<p style="text-align: justify;text-indent: 2em">The <em>median</em> represent similar information, as the value that splits the ordered data set in half. For an ordered sample \(x_{(1)} \le x_{(2)} \le \dots \le x_{(n)}\), the median \(M\) is defined in a different way if the list contains an even or odd number of observations. If \(n\) is even, meaning we can write it in terms of an arbitrary integer as \(n = 2k\), the median is just the middle-point
\[
M=x_{(k+1)} \; ,
\]
while if \(n\) is even, meaning that it can write it in terms of an arbitrary integer as \(n = 2k + 1\), the median is computed as the average of the two middle points
\[
M=\dfrac{x_{(k)} + x_{(k+1)}}{2} \; .
\]
This shifting definition we just wrote for \(M\) may seem a bit unnatural at first, so let's navigate it with a couple of examples. Consider the sample \(\mathcal{S} = \{1, 2, 3, 5, 3, 2, 7\}\). First, we order the data:
\[
\mathcal{S}_{\text{ordered}} = \{1, 2, 2, 3, 3, 5, 7\} \; .
\]
Since the sample has an odd number of elements \(n = 7\), the median is just the middle value:
\[
M = x_{(4)} = 3 \; .
\]
Now consider an even-sized sample \(\mathcal{S} = \{1, 2, 3, 5, 4, 3, 2, 7\}\). Ordering the data gives
\[
\mathcal{S}_{\text{ordered}} = \{1, 2, 2, 3, 3, 4, 5, 7\}.
\]
With has an even number of elements now, \(n = 8\). Hence, applying such case in our definition of \(M\), the median is the average of the two middle values:
\[
M = \frac{x_{(4)} + x_{(5)}}{2} = \frac{3 + 3}{2} = 3 \; .
\]
Unlike the mean, the median is robust to outliers and skewed data, capturing the central position of the dataset even with repeated values. For instance, the data represented in LHS of Figure \ref{fig:histogram_comparison1} will be accurately described by computing the mean, given its symmetric behaviour, while the one in the RHS will be better addressed with a median, accounting for the skewness and the presence of outliers.</p>
<p style="text-align: justify;text-indent: 2em">The <em>mode</em> is the value—or values—that appear most frequently in the observation set, which is quite a straightforward measure. For the first sample \(\mathcal{S} = \{1, 2, 3, 5, 3, 2, 7\}\) we just count the frequency of each value, and conclude that since both \(2\) and \(3\) occur most frequently, the dataset is <em>bimodal</em>, with modes \(2\) and \(3\). In the case of categorical data, such as eye colour or smartphone brands, the mode corresponds to the most common category.</p>
<p style="text-align: justify;text-indent: 2em">Beyond central location, it is important to understand the <em>spread</em> of the data. We can define the <em>variance</em> of a set, normally denoted by \(s^2\), as a quantity that captures how far are the elements from the mean value,
\[
s^2 = \frac{1}{n - 1} \sum_{i = 1}^{N} (x_{i} - \bar{x})^{2} \;
\]
and again, we will use a different notation for the <em>population variance</em>,
\[
\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2 \; .
\]
If we pay close attention, we see that the definitions of \(s^2\) and \(\sigma^{2}\) are not identical. The \(n-1\) in the denominator of the sample variance is called the <em>Bessel correction factor</em>, and it arises from the fact that treating finite samples is not the same as referring to the complete population. We will return to this topic in Chapter 3, when we discuss the concept of estimators and Maximum Likelihood Estimation.</p>
<p style="text-align: justify;text-indent: 2em">Note that the variance is just a sum of differences, and squared just so that we obtain a positive value. It is a measure starting with the first element (\(x_1\), for \(i = 1\)) and until the last one (\(x_N\), for \(i = N\)), of how far is each element from the mean value. If all elements in our sample are very close to the mean, then the sum of differences will be a small number, and we would get a variance \(s^2\) close to zero. Meanwhile, if the elements are very different, with large variation within the set, we would obtain a larger variance.</p>
<p style="text-align: justify;text-indent: 2em">Again, let's illustrate with an example. If we compute the variance of our very first example set \(\mathcal{S} = \{1, 2, 3\}\), which has just \(n = 3\) observations, we get
\[
s^2 = \frac{1}{3 - 1} \sum_{i = 1}^{3} (x_{i} - \bar{x})^{2} = \frac{1}{2} \big((1 - 2)^{2} + (2 - 2)^{2} + (2 - 3)^{2}\big) = \frac{1}{2} (1 + 0 + 1) = 1 \; ,
\]
which we could interpret as, on average, the elements of the list being \textit{one unit} away from the mean. As a warm up exercise, try to compute the variance for a second sample, let's say \(\mathcal{S} = \{4, 5, 6\}\). By substituting in the general expression of the sample variance you should get the following result
\[
s^2 = \frac{1}{3 - 1} \sum_{i = 1}^{3} (x_{i} - \bar{x})^{2} = \frac{1}{2} \big((4 - 5)^{2} + (5 - 5)^{2} + (6 - 5)^{2}\big) = \frac{1}{2} (1 + 0 + 1) = 1 \; .
\]
We obtain again a variance \(s^2 = 1\), indicating as in the previous example, that the elements of this sample \(\mathcal{S}\) are also <em>one unit</em> away from the mean.</p>
<p style="text-align: justify;text-indent: 2em">Another useful quantity used to characterize variability is the so called <em>standard deviation</em>, which is just the square root of the variance,
\[
s = \sqrt{\frac{1}{n - 1} \sum_{i = 1}^{n} (x_{i} - \bar{x})^{2}} \; ,
\]
and for the entire population,
\[
\sigma = \sqrt{\frac{1}{N } \sum_{i = 1}^{N} (x_{i} - \bar{x})^{2}} \; .
\]
The historical development of variance and standard deviation has roots in early statistical and error analysis work. The concept of variance was defined by Ronald Fisher in the early 1920s, though related ideas of squared deviations appeared earlier in Gauss’s work on error distributions when measuring star positions. The standard deviation, on the other hand, was named by Karl Pearson in 1890s, prior to which it was referred to as <em>root mean square error</em>. Pearson's naming made the concept more accessible and allowed widespread use for measuring data spread around the mean, upon which Fisher later built the idea of variance, as estimator of the variation within a sample.</p>
<p style="text-align: justify;text-indent: 2em">Finally, <em>quantiles</em> divide the ordered data into equal proportions. Mathematically, the \(p\)<sup>th</sup> quantile \(Q_p\) is the value below which a fraction \(p\) of the data lies. Special cases include the <em>first quartile</em> (\(Q_1\), or 25<sup>th</sup> percentile), the <em>median</em> (\(Q_2\), or 50<sup>th</sup> percentile), and the <em>third quartile</em> (\(Q_3\), or 75<sup>th</sup> percentile). Formally, for a continuous cumulative distribution function (CDF) \(F(x)\), the \(p\)<sup>th</sup> quantile satisfies
\[
Q_p = \inf \{ x : F(x) \ge p \}.
\]</p>
<p style="text-align: justify;text-indent: 2em">In summary, mean, median, mode, variance, standard deviation, and quantiles provide a rich, complementary view of the dataset’s central tendency and variability, allowing for both numerical and graphical summaries that capture the essence of the data.</p>
<p style="text-align: justify;text-indent: 2em">Variation is not merely a technicality; it is the very essence of uncertainty. Without spread, probability would be trivial, for every outcome would be the same. It is in the differences among observations that statistical inquiry finds its substance. Hence, central tendency and variation together provide the complementary lenses through which data becomes intelligible. They allow us to say whether two groups are alike or unlike, whether a new result is ordinary or surprising, whether the observed variation is too great to be dismissed as chance. In this sense, descriptive statistics foreshadows the inferential methods to come, hinting at deeper laws beneath the numbers.</p>
