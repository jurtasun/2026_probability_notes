<p class="p1" style="text-align: right;"><em>Statistics is the grammar of science</em></p>
<p class="p1" style="text-align: right;">— Karl Pearson</p>
&nbsp;

&nbsp;
<p class="indent" style="text-align: justify; text-indent: 2em;">A large part of history of science could be summarized as a continuous effort to translate observations of reality into precise, mathematical terms. To define mathematically the phenomena we find in the natural world, it is necessary to develop tools that express the one or more relevant quantities—sometimes called <em>variables—</em>and how they relate or change depending on one another. The purpose of such modelling might be, for instance, to determine the distance from the earth to the sun, to estimate the number of stars in the observable universe, or relating the number of lung cancer patients to pollution levels around smoking areas.</p>
<p style="text-align: justify; text-indent: 2em;">It has been said that mathematics could be summarized as all tasks related to <em>count</em>, <em>measure</em>, and <em>sort</em>. In a similar way, we could summarize all statistical issues as concern with <em>uncertainty</em>, or <em>variation</em> among observations. The word <em data-start="9" data-end="21">statistics</em> entered the English language through the work British politician and military officer John Sinclair, who in 1791 began publishing his monumental <em data-start="138" data-end="171">Statistical Account of Scotland, a </em>series of documentary publications covering life in Scotland in the 18th, 19th and 20th centuries. Adapting the German <em data-start="193" data-end="204">Statistik</em>—then denoting the descriptive study of states and their conditions—Sinclair expanded the term’s meaning to encompass “<em>an inquiry into the state of a country with respect to the happiness of its inhabitants</em>.” In doing so, he bridged the administrative traditional sense of the term with an empirical spirit that foreshadowed modern social science, into a discipline devoted to the systematic understanding of society itself. Through this chapter we will introduce terms such as description of populations, sampling, and chance. In further chapters we will develop and revisit ideas such as <em>randomness</em>, <em>relationship</em>, <em>correlation</em>, <em>confidence</em> and <em>reproducibility</em>, among others [...].</p>
<p style="text-align: justify; text-indent: 2em;">Historically, uncertainty has been associated with games of chance and gambling. The Royal Statistical Society, together with many other statistical groups, was originally set up to just <em>gather and publish data</em>, as an attempt to reduce its uncertainty. It remains an essential part of statistical activity today, and most Governments have statistical offices whose function is the plain acquisition and presentation of statistics. It did not take long before statisticians wondered how the data might best be used, and modern <em>statistical inference</em> was born.</p>
<p style="text-align: justify; text-indent: 2em;">The mathematical formalization of decision-making is actually quite a recent development. It is usually attributed to British mathematician Frank P. Ramsey (1903–1930), who in his 1926 paper <em>"Truth and Probability"</em> [...] introduced a formal, subjective interpretation of probability, laying the groundwork for what later became expected utility theory in decision-making under uncertainty. In short, Ramsey formalized how rational agents should assign probabilities and make decisions based on personal beliefs and preferences. All starting from the apparently simple question <em>"how should we make decisions in the face of uncertainty?"</em>.</p>
<p style="text-align: justify; text-indent: 2em;">Descriptive statistics gives us the tools to summarize, organize, and present data. By distilling observations into meaningful summaries, we find the first sense of order in apparent chaos. Descriptive statistics revolves around a few fundamental objects: data points, distributions, and summaries. Measures of central tendency—mean, median, mode—capture the “typical” observation, while measures of spread—variance, standard deviation, interquartile range—reveal how much observations fluctuate. Graphical tools such as histograms, box plots, and scatterplots turn abstract numbers into visible patterns, making insights immediate and intuitive. The development of measures like mean, median, and standard deviation emerged from this drive to quantify central tendencies and variability, giving rise to a shared language for describing empirical reality. The practical problems of descriptive statistics are deceptively simple: How do we summarize hundreds or thousands of observations? How do we detect patterns or outliers? How can we compare datasets rigorously? Each question demands careful choice of measure, sensitivity to data structure, and awareness of limitations. The challenge lies not in calculation alone, but in interpretation: a summary is meaningful only if it faithfully represents the underlying phenomena [...].</p>

<h3 style="text-align: justify;">1.1. Sampling and data types</h3>
<p class="p1 indent" style="text-align: justify; text-indent: 2em;">All statistical inquiries begin with observations and measurements, which we normally refer to as <em>data</em>. And data begins with the act of selection, or <em>sampling</em>. The natural world overflows with phenomena, offering endless opportunities for observation, but only a finite subset can ever be recorded. This distinction gives rise to two central notions: the <em>population</em>, which we will write as \(\mathcal{P}\), and the <em>sample</em>, denoted by \(\mathcal{S}\). By <em>population</em> we mean the complete set of all possible observations under study, normally written as
\[
\mathcal{P} = \{x_1, x_2, \dots, x_N\} \; .
\]</p>
<p style="text-align: justify;">The <em>sample</em>, on the other hand, is the finite subset actually collected. For a series of \(N\) observations \(x_1\), \(x_2\), ..., \(x_N\), a sample of just \(n\) elements—less than the total, which is normally denoted by the upper case \(N\)—is defined as
\[
\mathcal{S} = \{\chi_1, \chi_2, \dots, \chi_n\}, \quad n &lt; N \; ,
\]</p>
<p style="text-align: justify;">where the \(\chi_i\)-labels remind us that the sample consists of selected observations from the population, not necessarily consecutive or all of them. The population represents the ideal object of inference, while the sample is the concrete, finite evidence available to us. This distinction is far from trivial; a poorly chosen sample often misrepresents the population and may induce bias, whereas a carefully constructed one mirrors its essential features, and can be used to describe the underlying nature.</p>
<p style="text-align: justify; text-indent: 2em;">Equally important is the recognition that not all data are of the same kind. A common distinction is to consider <em>categorical</em> and <em>numerical</em> data. Categorical—or <em>qualitative—</em>data describes qualities or labels such as the eye colour of students in a classroom (blue, brown, green), the brand of a smartphone, etc. Sometimes they are further divided into <em>nominal</em> categories, with no natural order, like the eye colour or the smartphone brand, and <em>ordinal</em> categories with a meaningful order. Examples of these would be the finishing places in a race (first, second, third), survey responses ranging from <em>strongly disagree</em> to <em>strongly agree</em>, etc.</p>
<p style="text-align: justify; text-indent: 2em;">The other big group is normally referred to as numerical—or <em>quantitative—</em>data. These measure numerical quantities and are often subdivided into <em>discrete</em>, countable numbers, such as the number of books on a shelf (4, 5, 6) or the number of goals scored in a match, and <em>continuous</em> values that can take any number within a range, such as the time a sprinter takes to run 100 meters, or the height of a person measured with some arbitrary precision.</p>
<p class="indent" style="text-align: justify;">Distinguishing between these types is no mere slang; different types of observations require different mathematical tools, and will be described in different ways. For example, it would not make sense to compute a mean out of smartphone brands, but to compute the mean of their prices is informative. Similarly, the distribution of finishing places after a race might be summarized by a median position, whereas heights of athletes could be studied with averages and measures of spread. A correct classification of data is thus a safeguard against misuse and a guide toward insight.</p>
<p style="text-align: justify; text-indent: 2em;">As a summary, sampling and proper description of data establish the ground upon which statistics is built. Before calculating, summarizing, or diving into inference, one must ensure that the information collected is both representative and properly understood. Without these foundations, descriptive measures risk floating unmoored, detached from the reality they claim to represent. Accurate sampling and rigorous description will lead to a faithful representation of the phenomena under study and their relationships, detecting anomalies, and even building accurate predictions.</p>
<p style="text-align: justify; text-indent: 2em;">In the recent decades, experimental scientists have become aware of a certain tendency to use statistics as a crutch, relying on them for validation rather than seeking genuine understanding. Scottish poet Andrew Lang marvellously summarizes this tren in his famous quote <em>"most people use statistics as a drunken man uses lamp-posts—for support rather than illumination". </em>Lang's observation serves as a cautionary reminder to approach statistical data with critical thinking and not merely as a tool to bolster preconceived notions.</p>

<div style="text-align: justify;"></div>
<div style="text-align: justify;">
<h3>1.2. Central tendency and variation</h3>
<p style="text-align: justify; text-indent: 2em;">Once observations have been collected, a natural question arises: what is the <em>central</em>, or <em>typical</em> value of this data set? Mathematical quantities that measure the central tendency will be useful to summarize our data with a single representative number, providing an immediate sense of location within the distribution.</p>
<p style="text-align: justify; text-indent: 2em;">The <em>mean</em>, or <em>average</em> is perhaps the most familiar measure of central tendency. Imagine we are doing an experiment where we measure some variable, and let's call it \(x\) for simplicity. \(x\) can be anything we could measure, like number of tomatoes in a bag, position at a given time, energy of some system, concentration of a specific substance, etc. Let's imagine we repeat the measurement \(n\) times, and we obtain the values \(x_1, x_2, \dots, x_n\). That will be our set of observations, or our <em>sample</em> \(\mathcal{S}\). We could simply write it as a list—or a <em>vector—</em>in the following way:</p>
\[
\mathcal{S} = \{x_1, x_2, \dots, x_n\} \; .
\]
<p style="text-align: justify;">Keep in mind that from the mathematics perspective the word <em>vector</em> has a slightly different meaning, with subtleties related to algebraic operations and relations they should satisfy, but for the purpose of this course, where we prioritize above all simplicity, a vector and a list of numbers will be essentially the same thing.</p>
<p style="text-align: justify;">We can define a quantity called the <em>mean—</em>or <em>average</em>—of an arbitrary large sample of \(n\) observations, as the sum of all elements divided by the total. We will write it as \(\bar{x}\), and define it as follows:\[
\bar{x} = \frac{1}{n} (x_{1} + x_{2} + ... + x_{n}) \; .
\]</p>
We can write this in a slightly more compact way as a <em>summation</em>, as follows:

\[
\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i \; .
\]
<p style="text-align: justify;">Here we denote the sum of all elements \(x_{i}\) with the greek letter \(\Sigma\), starting with the first one (\(x_1\), for \(i = 1\)) and until the last one (\(x_n\), for \(i = n\)). Both definitions for \(\bar{x}\) mean <em>exactly</em> the same thing, just written in different ways.</p>
<p style="text-align: justify; text-indent: 2em;">Let's pause here for a second, and give a note about notation. Remember the difference we made at the very beginning between sample and population, as notations may differ between different books and literature sources. Normally, the sample mean is written just as \(\bar{x]\) just as we did, while for the full population of \(N\) elements \(x_1, x_2, \dots, x_N\)—before any sampling—the <em>population mean</em> is normally denoted as \(\mu\), and defined accordingly.</p>
\[
\mu = \frac{1}{N} \sum_{i = 1}^{N} x_{i} \;
\]

We will see more about the difference between sample mean and population mean when we discuss parameter estimation in Chapter 3. For now just keep in mind that \(\bar{x}\) is the mean of our sample of just \(n\) drawn observations, while \(\mu\) refers to the mean of the idealized, complete population.
<p style="text-align: justify; text-indent: 2em;">Let's illustrate with an example. Suppose we repeat a measurement three times, obtaining the results \(x_1 = 1\), \(x_2 = 2\), and \(x_3 = 3\). Our sample is then \(\mathcal{S} = \{1, 2, 3\}\), and the sample mean is</p>
\[
\bar{x} = \frac{1}{3} \sum_{i = 1}^{3} x_{i} = \frac{1}{3} (1 + 2 + 3) = 2 \; .
\]
As a warm-up exercise, try computing the same mean value for a second sample, let's say \(\textbf{x} = \{4, 5, 6\}\). Substituting into the general expression we obtain
\[
\bar{x} = \frac{1}{3} \sum_{i = 1}^{3} x_{i} = \frac{1}{3} (4 + 5 + 6) = 5 \; .
\]
The mean captures information about the "central" value, where most events cluster. Although useful, it is sensitive to extreme values or <em>outliers</em>, which motivates the definition additional, more robust measures of central tendency.
<p style="text-align: justify; text-indent: 2em;">The <em>median</em> represent similar information, as the value that splits the ordered data set in half. For an ordered sample \(x_{(1)} \le x_{(2)} \le \dots \le x_{(n)}\), the median \(M\) is defined as</p>
\[
M=x_{(k+1)} \; \text{ if } n = 2k+1 \text{(odd)} \; ,
\]
\[
M=\dfrac{x_{(k)} + x_{(k+1)}}{2} \; \text{ if } n = 2k \text{(even)} \; .
\]
Note that here \(k\) is just an integer that helps locate the middle position of an ordered data set of size \(n\). If the sample size \(n\) is even, we write \(n = 2k\), while for \(n\) odd, we write \(n = 2k + 1\). In the case of an odd-sized sample, the median is just the middle-point, while for an even size, it is computed as the average of the two middle points. The mathematical definition we just wrote for \(M\) may seem a bit unnatural at first, so let's navigate it with a couple of examples. Consider the sample \(\mathcal{S} = \{1, 2, 3, 5, 3, 2, 7\}\). First, we order the data:
\[
\mathcal{S}_{\text{ordered}} = \{1, 2, 2, 3, 3, 5, 7\} \; .
\]
Since the sample has an odd number of elements ($n = 7$), the median is just the middle value:
\[
M = x_{(4)} = 3 \; .
\]
Now consider an even-sized sample \(\mathcal{S} = \{1, 2, 3, 5, 4, 3, 2, 7\}\). Ordering the data gives
\[
\mathcal{S}_{\text{ordered}} = \{1, 2, 2, 3, 3, 4, 5, 7\}.
\]
With has an even number of elements now, \(n = 8\). Hence, applying such case in our definition of \(M\), the median is the average of the two middle values:
\[
M = \frac{x_{(4)} + x_{(5)}}{2} = \frac{3 + 3}{2} = 3 \; .
\]
Unlike the mean, the median is robust to outliers and skewed data, capturing the central position of the dataset even with repeated values. For instance, the data represented in LHS of Figure \ref{fig:histogram_comparison1} will be accurately described by computing the mean, given its symmetric behaviour, while the one in the RHS will be better addressed with a median, accounting for the skewness and the presence of outliers.
<p style="text-align: justify; text-indent: 2em;">The <em>mode</em> is the value—or values—that appear most frequently in the observation set, which is quite a straightforward measure. For the first sample \(\mathcal{S} = \{1, 2, 3, 5, 3, 2, 7\}\) we just count the frequency of each value, and conclude that since both \(2\) and \(3\) occur most frequently, the dataset is <em>bimodal</em>, with modes \(2\) and \(3\). In the case of categorical data, such as eye colour or smartphone brands, the mode corresponds to the most common category.</p>
<p style="text-align: justify; text-indent: 2em;">Beyond central location, it is important to understand the <em>spread</em> of the data. We can define the <em>variance</em> \(s^2\) of a set as a quantity that captures how far are the elements from the mean value,
\[
s^2 = \frac{1}{n - 1} \sum_{i = 1}^{N} (x_{i} - \bar{x})^{2} \;
\]
and again, we will use a different notation for the <em>population variance</em>
\[
\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2 \; .
\]
If we pay close attention, we see that the definitions of \(s^2\) and \(\sigma^{2}\) are not identical. The \(n-1\) in the denominator of sample variance is called the <em>Bessel correction factor</em>, and it arises from the fact that treating finite samples is not the same as referring to the complete population. We will return to this topic in Chapter 3, when we discuss the concept of estimators and Maximum Likelihood Estimation.</p>
<p style="text-align: justify; text-indent: 2em;">Note that the variance is just a sum of differences, and squared just so that we obtain a positive value. It is a measure starting with the first element (\(x_1\), for \(i = 1\)) and until the last one (\(x_N\), for \(i = N\)), of how far is each element from the mean value. If all elements in our sample are very close to the mean, then the sum of differences will be a small number, and we would get a variance \(s^2\) close to zero. Meanwhile, if the elements are very different, we would obtain a larger variance.</p>
<p style="text-align: justify; text-indent: 2em;">Again, let's illustrate with an example. If we compute the variance of our very first example set $\textbf{x} = \{1, 2, 3\}$, which has just $n = 3$ observations, we get
\[
s^2 = \frac{1}{3 - 1} \sum_{i = 1}^{3} (x_{i} - \bar{x})^{2} = \frac{1}{2} \big((1 - 2)^{2} + (2 - 2)^{2} + (2 - 3)^{2}\big) = \frac{1}{2} (1 + 0 + 1) = 1 \; ,
\]
which we could interpret as, on average, the elements of the list being \textit{one unit} away from the mean.</p>
<p style="text-align: justify; text-indent: 2em;">As a warm up exercise, try to compute the variance for a second sample, let's say $\textbf{x} = \{4, 5, 6\}$. By substituting in the general expression \eqref{eq:sample_variance} you should get the result
\[
s^2 = \frac{1}{3 - 1} \sum_{i = 1}^{3} (x_{i} - \bar{x})^{2} = \frac{1}{2} \big((4 - 5)^{2} + (5 - 5)^{2} + (6 - 5)^{2}\big) = \frac{1}{2} (1 + 0 + 1) = 1 \; .
\]
We obtain again a variance $s^2 = 1$, indicating as in the previous example, that the elements of this sample $\textbf{x}$ are also \textit{one unit} away from the mean.</p>
<p style="text-align: justify; text-indent: 2em;">Another useful quantity used to characterize variability is the so called \textit{standard deviation}, which is just the square root of the variance,
\[
s = \sqrt{\frac{1}{n - 1} \sum_{i = 1}^{n} (x_{i} - \bar{x})^{2}} \; ,
\]
and for the entire population,
\[
\sigma = \sqrt{\frac{1}{N } \sum_{i = 1}^{N} (x_{i} - \bar{x})^{2}} \; .
\]
At a glance, variance and standard deviation quantify how much the elements of a dataset deviate from the mean, capturing the notion of \textit{spread}.</p>
<p style="text-align: justify; text-indent: 2em;">Finally, \textit{quantiles} divide the ordered data into equal proportions. The $p$-th quantile $Q_p$ is the value below which a fraction $p$ of the data lies. Special cases include the \textit{first quartile} ($Q_1$, 25th percentile), the \textit{median} ($Q_2$, 50th percentile), and the \textit{third quartile} ($Q_3$, 75th percentile). Formally, for a continuous cumulative distribution function (CDF) $F$, the $p$-th quantile satisfies
\[
Q_p = \inf \{ x : F(x) \ge p \}.
\]</p>
<p style="text-align: justify; text-indent: 2em;">In summary, mean, median, mode, variance, standard deviation, and quantiles provide a rich, complementary view of the dataset’s central tendency and variability, allowing for both numerical and graphical summaries that capture the essence of the data.</p>
<p style="text-align: justify; text-indent: 2em;">Variation is not merely a technicality; it is the very essence of uncertainty. Without spread, probability would be trivial, for every outcome would be the same. It is in the differences among observations that statistical inquiry finds its substance. Hence, central tendency and variation together provide the complementary lenses through which data becomes intelligible. They allow us to say whether two groups are alike or unlike, whether a new result is ordinary or surprising, whether the observed variation is too great to be dismissed as chance. In this sense, descriptive statistics foreshadows the inferential methods to come, hinting at deeper laws beneath the numbers.</p>
<p style="text-align: justify; text-indent: 2em;">In summary, mean, median, mode, variance, standard deviation, and quantiles provide a rich, complementary view of the dataset’s central tendency and variability, allowing for both numerical and graphical summaries that capture the essence of the data.</p>
<p style="text-align: justify; text-indent: 2em;">Variation is not merely a technicality; it is the very essence of uncertainty. Without spread, probability would be trivial, for every outcome would be the same. It is in the differences among observations that statistical inquiry finds its substance. Hence, central tendency and variation together provide the complementary lenses through which data becomes intelligible. They allow us to say whether two groups are alike or unlike, whether a new result is ordinary or surprising, whether the observed variation is too great to be dismissed as chance. In this sense, descriptive statistics foreshadows the inferential methods to come, hinting at deeper laws beneath the numbers.</p>

<div style="justify-content: center; align-items: flex-start; gap: 20px; margin-top: 10px;">
<figure style="text-align: center;"><img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram_1-300x191.png" alt="Symmetric Gaussian" width="300" height="191" /><figcaption>Symmetric Gaussian</figcaption></figure>
<figure style="text-align: center;"><img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram_2-300x191.png" alt="Skewed Gaussian" width="300" height="191" /><figcaption>Skewed Gaussian</figcaption></figure>
</div>
<div style="justify-content: space-between; align-items: flex-start; gap: 20px; flex-wrap: nowrap; width: 100%; margin-top: 10px;">
<figure style="flex: 1; text-align: center;"><img style="max-width: 100%; height: auto;" src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram_1-300x191.png" alt="Symmetric Gaussian" /><figcaption>Symmetric Gaussian</figcaption></figure>
<figure style="flex: 1; text-align: center;"><img style="max-width: 100%; height: auto;" src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram_2-300x191.png" alt="Skewed Gaussian" /><figcaption>Skewed Gaussian</figcaption></figure>
</div>
<h3>1.3. Data visualization</h3>
Data visualization [...].

</div>
<p style="text-align: justify;"><strong>1.4 Dependency, linearity, correlation</strong></p>
<p style="text-align: justify;">Dependency, linearity, correlation [...].</p>